{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3810jvsc74a57bd09be7d832d8de1cb41954ddb88c6cff15fa80094fc3da20a455e5d7ac3921d376",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "metadata": {
   "interpreter": {
    "hash": "9be7d832d8de1cb41954ddb88c6cff15fa80094fc3da20a455e5d7ac3921d376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unicode_to_ascii(s):\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                 if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "  # creating a space between a word and the punctuation following it\n",
    "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "  w = re.sub(r'[\" \"]+', \" \", w)\n",
    "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "  w = w.strip()\n",
    "  # adding a start and an end token to the sentence\n",
    "  # so that the model know when to start and stop predicting.\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w\n",
    "\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "  return tensor, lang_tokenizer\n",
    "\n",
    "fileContent=\"你好\\tyou are good\\n你坏\\tyou are bad\\n你特别好\\tyou are very good\\n你特别坏\\tyou are very bad\"\n",
    "def create_dataset(path, num_examples):\n",
    "  #lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "  lines = fileContent.strip().split('\\n')\n",
    "  word_pairs = [[preprocess_sentence(w) for w in line.split('\\t')]\n",
    "                for line in lines[:num_examples]]\n",
    "  return zip(*word_pairs)\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "  # creating cleaned input, output pairs\n",
    "  targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer\n",
    "\n",
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 30000\n",
    "#input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file,num_examples)\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('no',num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t != 0:\n",
    "      print(f'{t} ----> {lang.index_word[t]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.batch_sz = batch_sz\n",
    "    self.enc_units = enc_units\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    print(f'done init gru ')\n",
    "  def call(self, x, hidden):\n",
    "    print(f'in call ')\n",
    "    print(x)\n",
    "    x = self.embedding(x)\n",
    "    print(f'after embedding')\n",
    "    print(x)\n",
    "    output, state = self.gru(x, initial_state=hidden)\n",
    "    return output, state\n",
    "  def initialize_hidden_state(self):\n",
    "    return tf.zeros((self.batch_sz, self.enc_units))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "done init gru \n",
      "done init gru \n",
      "in call \n",
      "tf.Tensor(\n",
      "[[1 2 3 5 4 0]\n",
      " [1 2 3 7 5 4]], shape=(2, 6), dtype=int32)\n",
      "after embedding\n",
      "tf.Tensor(\n",
      "[[[ 0.02373388 -0.04817441  0.00190314]\n",
      "  [ 0.01570146  0.03898109 -0.01314337]\n",
      "  [-0.02908884 -0.03115588  0.04987849]\n",
      "  [ 0.01183702 -0.04894618  0.01673203]\n",
      "  [ 0.00622917  0.01814096 -0.0026949 ]\n",
      "  [ 0.03724725 -0.00412703 -0.01082895]]\n",
      "\n",
      " [[ 0.02373388 -0.04817441  0.00190314]\n",
      "  [ 0.01570146  0.03898109 -0.01314337]\n",
      "  [-0.02908884 -0.03115588  0.04987849]\n",
      "  [-0.02108764  0.03351064 -0.02924147]\n",
      "  [ 0.01183702 -0.04894618  0.01673203]\n",
      "  [ 0.00622917  0.01814096 -0.0026949 ]]], shape=(2, 6, 3), dtype=float32)\n",
      "Encoder output shape: (batch size, sequence length, units) (2, 6, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (2, 1024)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BUFFER_SIZE = len(input_tensor)\n",
    "BATCH_SIZE = 2\n",
    "steps_per_epoch = len(input_tensor)//BATCH_SIZE\n",
    "embedding_dim = 3  ### originally 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor, target_tensor)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "\n",
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, 2)\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units)', sample_output.shape)\n",
    "print('Encoder Hidden state shape: (batch size, units)', sample_hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sample_hidden' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5014641b0857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mattention_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBahdanauAttention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[0mattention_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_hidden\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Attention result shape: (batch size, units)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_hidden' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super(BahdanauAttention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "  def call(self, query, values):\n",
    "    # query hidden state shape == (batch_size, hidden size)\n",
    "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "    # values shape == (batch_size, max_len, hidden size)\n",
    "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "    query_with_time_axis = tf.expand_dims(query, 1)\n",
    "    # score shape == (batch_size, max_length, 1)\n",
    "    # we get 1 at the last axis because we are applying score to self.V\n",
    "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "    score = self.V(tf.nn.tanh(\n",
    "        self.W1(query_with_time_axis) + self.W2(values)))\n",
    "    # attention_weights shape == (batch_size, max_length, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * values\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units)\", attention_result.shape)\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1)\", attention_weights.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}